{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "invalid-prescription",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "confidential-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "piano-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 17:48:33,215\tWARNING utils.py:480 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2022-01-24 17:48:33,754\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-01-24 17:48:33,761\tWARNING services.py:1640 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "workdir = os.path.join(os.getenv('HOME'), 'aiffel/mpii')\n",
    "os.chdir(workdir)\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "ray.shutdown()\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-frontier",
   "metadata": {},
   "source": [
    "### json 파싱\n",
    "\n",
    "- json 파일들은 이미지에 담겨있는 사람의 pose keypoint 정보들을 가지고 있어서 Pose Estimation을 위한 label로 삼을 수 있다.\n",
    "\n",
    "- json 파일을 열어서 샘플로 annotation정보를 1개만 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "breathing-pointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "json_file_path = os.getenv('HOME')+'/aiffel/mpii/mpii_human_pose_v1_u12_2/train.json'\n",
    "\n",
    "with open(json_file_path) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2)\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-beginning",
   "metadata": {},
   "source": [
    "`joints`가 label로 사용할 Keypoint의 label.\n",
    "\n",
    "이미지 형상과 사람의 포즈에 따라 모든 label이 이미지에 나타나지 않기 때문에 `joints_vis`를 이용해서 실제로 사용할 수 있는 keypoint인지 나타낸다.\n",
    "\n",
    "MPII의 경우 1(visible)/0(non) 으로만 나누어지기 때문에 쉽다.\n",
    "\n",
    "coco의 경우 2/1/0 으로 표현하여 occlusion 상황까지 label화 되어 있다.\n",
    "\n",
    "#### `joints` 순서는\n",
    "\n",
    "- 0 - 오른쪽발목\n",
    "- 1 - 오른쪽 무릎\n",
    "- 2 - 오른쪽 엉덩이\n",
    "- 3 - 왼쪽엉덩이\n",
    "- 4 - 왼쪽무릎\n",
    "- 5 - 왼쪽발목\n",
    "- 6 - 골반\n",
    "- 7 - 가슴\n",
    "- 8 - 목\n",
    "- 9 - 머리 위\n",
    "- 10 - 오른쪽 손목\n",
    "- 11 - 오른쪽 팔꿈치\n",
    "- 12 - 오른쪽 어깨\n",
    "- 13 - 왼쪽 어깨\n",
    "- 14 - 왼쪽 팔꿈치\n",
    "- 15 - 왼쪽 손목\n",
    "\n",
    "\n",
    "#### `scale`과 `center`\n",
    "\n",
    "- 높이 = scale * 200px\n",
    "- center는 사람의 중심점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "turkish-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json annotation을 파싱하는 함수\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename':filename,\n",
    "        'filepath':os.path.join(image_dir, filename),\n",
    "        'joints_visibility':joints_visibility,\n",
    "        'joints':joints,\n",
    "        'center':anno['center'],\n",
    "        'scale':anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-syracuse",
   "metadata": {},
   "source": [
    "- image의 전체 path를 묶어 dict 타입의 label로 만들어낸다.\n",
    "- 이 label을 가지고 학습을 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-execution",
   "metadata": {},
   "source": [
    "# tfrecord\n",
    "\n",
    "### tfrecord 파일 만들기\n",
    "\n",
    "- 실제 프로젝트에서는 튜토리얼 데이터셋보다 훨씬 큰 크기의 데이터를 다뤄야하는데 일반적으로 학습과정에서 gpu의 연산속도보다 HDD I/O가 느리기 때문에 병목 현상이 발생하고 실험효율성이 떨어진다.\n",
    "\n",
    "#### 학습데이터를 어떻게 빠르게 읽는가? ==> 중요\n",
    "\n",
    "- data read(또는 prefetch) 또는 데이터 변환단계\n",
    "- gpu 학습과 병렬적으로 수행되도록 prefetch를 적용해야한다.\n",
    "- 수행방법은 tf.data 의 map함수이용, cache에 저장해두는 방법사용\n",
    "\n",
    "tensorflow에서는 데이터셋을 tfrecord 형태로 표현하기 위한 자동화도구 제공.\n",
    "\n",
    "tfrecord는 binary record sequence를 저장하기 위한 형식\n",
    "\n",
    "내부적으로 protocol buffer 라는 것을 이용\n",
    "#### [protocol buffer](https://developers.google.com/protocol-buffers/?hl=ko)\n",
    "\n",
    "- protobuf는 직렬화 데이터 라이브러리\n",
    "- 데이터셋 크기가 크기때문에 빠른 학습을 위해서 이 정보를 tfrecord 파일로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "offshore-astronomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecord 파일로 변환\n",
    "import ray\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(split,\n",
    "                                                                   str(i+1).zfill(4),\n",
    "                                                                   str(total_shards).zfill(4))) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-advancement",
   "metadata": {},
   "source": [
    "#### 용어 정리\n",
    "- annotation 을 total_shards 개수로 나눈다.(chunkify)(train:64개, val:8개)\n",
    "- build_single_tfrecord 함수를 통해 tfrecord로 저장\n",
    "- 각 chunk끼리 dependency가 없기 때문에 병렬처리가능, ray를 사용\n",
    "\n",
    "\n",
    "#### annotation을 shard로 나눠야하는 이유?\n",
    "- I/O 병목을 피하기 위해 입력 파일을 여러개로 나눈 뒤, 병렬적으로 prefetch하는 것이 학습 속도를 빠르게한다.\n",
    "- 데이터를 읽는 호스트보다 최소 10배 많은 파일을 보유하는것이 좋다. 동시에 각 파일은 I/O prefetch의 이점을 누릴 수 있도록 충분히 커야한다.(최소 10MB이상, 이상적으로는 100MB 이상)\n",
    "\n",
    "\n",
    "\n",
    "튜토리얼대로 annotation을 적절한 개수로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "directed-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotation을 적절한 개수로 나누는 함수\n",
    "def chunkify(l,n):\n",
    "    size = len(l)//n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n-1):\n",
    "        results.append(l[start:start+size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-amendment",
   "metadata": {},
   "source": [
    "- l은 annotation, n은 shards개수\n",
    "- shard개수 단위로 annotation list를 나누어서 새로운 list를 만든다.\n",
    "- numpy array라고 가정하면 (size, shard, anno_content) 정도의 shape을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conventional-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecord 1개를 저장하는 함수\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for' + path)\n",
    "    \n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "    print('finished building tf records for'+path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-gateway",
   "metadata": {},
   "source": [
    "- TFRecordWriter를 이용하여 anno_list를 shard개수 단위로 작성\n",
    "- generate_tfexample 함수를 사용\n",
    "- **write 할 때 string으로 serialize해야한다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alive-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.example\n",
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-display",
   "metadata": {},
   "source": [
    "- 정의한 json의 python type의 값들을 tfexample에 사용할 수 있는 값으로 변환\n",
    "- image 파일은 byte로 변환. bitmap으로 저장하게 되면 파일 용량이 커지므로 jpeg 타입이 아닌 경우 jpeg로 변화 후 content로 불러서 저장\n",
    "- 각 label값을 tf.train.Feature로 저장. 데이터타입에 주의\n",
    "- 이미지는 byte 인코딩 된 값을 그대로 넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hybrid-fault",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-intermediate",
   "metadata": {},
   "source": [
    "# Ray\n",
    "\n",
    "- Ray는 파이썬을 위한 간단한 분산 애플리케이션 API\n",
    "\n",
    "#### multiprocessing과 Ray의 사용상 차이점은?\n",
    "- multiprocenssing : 병렬화를 위해 추상적 구조를 새로 설계\n",
    "- Ray : 쓰던 코드에서 거의 수정없이 병렬화 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "further-breakdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 17:48:38,729\tWARNING utils.py:480 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.\n",
      "2022-01-24 17:48:39,165\tINFO services.py:1173 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "2022-01-24 17:48:39,173\tWARNING services.py:1640 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This may slow down performance! You may be able to free up space by deleting files in /dev/shm or terminating any running plasma_store_server processes. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=630)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=629)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n",
      "\u001b[2m\u001b[36m(pid=631)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "# tfrecords_mpii.py\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    # x = [\n",
    "    #     joint[0] / width if joint[0] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    # y = [\n",
    "    #     joint[1] / height if joint[1] >= 0 else joint[0]\n",
    "    #     for joint in anno['joints']\n",
    "    # ]\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        # 'image/object/parts/x':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=x)),\n",
    "        # 'image/object/parts/y':\n",
    "        # tf.train.Feature(float_list=tf.train.FloatList(value=y)),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)\n",
    "\n",
    "\n",
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)\n",
    "\n",
    "\n",
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def main():\n",
    "    print('Start to parse annotations.')\n",
    "    if not os.path.exists('./tfrecords_mpii'):\n",
    "        os.makedirs('./tfrecords_mpii')\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/train.json') as train_json:\n",
    "        train_annos = json.load(train_json)\n",
    "        train_annotations = [\n",
    "            parse_one_annotation(anno, './images/')\n",
    "            for anno in train_annos\n",
    "        ]\n",
    "        print('First train annotation: ', train_annotations[0])\n",
    "        del (train_annos)\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/validation.json') as val_json:\n",
    "        val_annos = json.load(val_json)\n",
    "        val_annotations = [\n",
    "            parse_one_annotation(anno, './images/') for anno in val_annos\n",
    "        ]\n",
    "        print('First val annotation: ', val_annotations[0])\n",
    "        del (val_annos)\n",
    "\n",
    "    print('Start to build TF Records.')\n",
    "    build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "    build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "    print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "        len(train_annotations) + len(val_annotations)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-venue",
   "metadata": {},
   "source": [
    "`cd ~/mpii_folder && python tfrecords_mpii.py` 를 터미널을 통해 수행한다,10분예상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "recognized-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     72      72    2072\r\n"
     ]
    }
   ],
   "source": [
    "# 200Mb 정도의 tfrecords들이 72개 만들어짐.\n",
    "!cd ~/aiffel/mpii/tfrecords_mpii && ls | wc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-subscription",
   "metadata": {},
   "source": [
    "# Data label 로 만들기\n",
    "\n",
    "tfrecords 파일을 읽고 전처리를 할 수 있는 dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sized-dragon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y)\n",
    "\n",
    "        # print (image.shape, heatmaps.shape, type(heatmaps))\n",
    "\n",
    "        return image, heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-vegetation",
   "metadata": {},
   "source": [
    "`Preprocessor` 클래스 코드에서 `__call__()` 메소드 내부에서 진행되는 주요 과정을 정리하면\n",
    "\n",
    "- tfrecord 파일이기 때문에 병렬로 읽는 것은 tf가 지원. `self.parse_tfexample()`에 구현되어있고, 이 함수를 통해 `tf.tensor`로 이루어진 dictionary형태의 `features`를 얻을 수 있다.\n",
    "- image는 `features['image/encoded']` 형태로 사용할 수 있고 tfrecord를 저장할 때 jpeg encoding 된 값을 넣었으므로 `tf.io.decode_jpeg()`로 decoding 하여 tensor 형태의 이미지를 얻는다.\n",
    "- `crop_roi()` 메소드를 이용해 해당 이미지를 학습하기 편하도록 트릭적용\n",
    "- `make_heatmaps()` 메소드로 label을 heatmap으로 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "academic-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfexample(self, example_proto):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'imaage/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-tomato",
   "metadata": {},
   "source": [
    "- tfrecord 파일 형식을 저장한 data type feature에 맞게 parsing한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
